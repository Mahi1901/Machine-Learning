{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN sentiment analysis-Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlDXMoy0O74F",
        "outputId": "08bc2c2c-4ecc-49c2-e9a3-55fe2f8ee74d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "w0CuA2Y7PC2_",
        "outputId": "6343da80-3fc5-4e23-f38f-32545a38794e"
      },
      "source": [
        "import pandas as pd \n",
        "df=pd.read_csv(\"/content/drive/MyDrive/IMDB Dataset.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd7kpy5dPqkR",
        "outputId": "63dff4f2-dd01-4725-c588-f70d3b4faa9c"
      },
      "source": [
        "\n",
        "# Import the dataset from keras.datasets module \n",
        "from keras.datasets import imdb\n",
        "\n",
        "# Limit the maximum size of the vocabulary to VOCAB_SIZE\n",
        "# Only top VOCAB_SIZE most frequently occurring words in the dataset will be considered\n",
        "# Discarding rarely words would help keep a manageable size\n",
        "# Words in the training set which are not among the top VOCAB_SIZE most frequent words, will be marked by the 'out of vocabulary' character.\n",
        "# Words occurring in the test set (but not in the training set) which are not among the top VOCAB_SIZE most frequent words are skipped.\n",
        "VOCAB_SIZE = 20000\n",
        "# Skip the most frequently occurring SKIP_COUNT\n",
        "# These words are often articles/determiner/prepositions (Eg. a, the, an etc) and hence not helpful classifying the reviews\n",
        "SKIP_COUNT = 0\n",
        "# Filter out sentences which are longer than MAX_LENGTH\n",
        "MAX_LENGTH = None\n",
        "\n",
        "print(\"Loading the IMDB dataset...\")\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=VOCAB_SIZE, skip_top=SKIP_COUNT, maxlen=MAX_LENGTH)\n",
        "print(\"Dataset loaded!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the IMDB dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset loaded!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exQ3IpKHQOEu",
        "outputId": "23cd3a4f-54fa-436b-dd01-cd33ad9ea356"
      },
      "source": [
        "print(\"Size of training set :\", len(train_data))\n",
        "print(\"Size of test set     :\", len(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set : 25000\n",
            "Size of test set     : 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hws3iK0vQnl7",
        "outputId": "cea90c19-3450-496b-d8cd-3888efd0ac8f"
      },
      "source": [
        "print(\"Sample training example:\")\n",
        "print(train_data[3])\n",
        "print(\"Label:\")\n",
        "print(train_labels[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample training example:\n",
            "[1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 2, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 2, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 2, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 2, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 2, 217, 4122, 1710, 537, 2, 1236, 5, 736, 10, 10, 61, 403, 9, 2, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 2, 4, 538, 7, 1795, 246, 2, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 2, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 2, 21, 29, 9, 2841, 23, 4, 1010, 2, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 2, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 2, 7, 31, 7, 2, 91, 2, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 2, 4, 2, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 2, 14, 280, 13, 219, 4, 2, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 2, 4, 15812, 804, 2, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 2, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574]\n",
            "Label:\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-90KQy31QppU",
        "outputId": "55242b15-eee6-4213-f05c-faf6df9a580f"
      },
      "source": [
        "# Get the mapping from each word in vocabulary to its index\n",
        "word_to_index = imdb.get_word_index()\n",
        "\n",
        "index_to_word = dict ( [(idx, word) for (word, idx) in word_to_index.items()] )\n",
        "\n",
        "\n",
        "# Check the review in English too\n",
        "english_review = ' '.join ( [index_to_word.get(idx-3, '?') for idx in train_data[3]] )\n",
        "# NOTE: In the dataset,\n",
        "#       index 0 is reserved for \"padding\",\n",
        "#       index 1 is reserved for \"start of sequence\"\n",
        "#       index 2 is reserved for \"unknown\"\n",
        "#\n",
        "# However, this is not followed in the word-to-index mapping obtained from imdb.get_word_index()\n",
        "# In the mapping, index 0 is for None, and thereafter, index i maps to the word at index i-3 (i.e. \"padding\", \"start of sequence\" and \"unknown\" are skipped in the mapping)\n",
        "# Hence the change in the above line of code \n",
        "\n",
        "print(\"Earlier training example in English:\")\n",
        "print(english_review)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "Earlier training example in English:\n",
            "? the scots excel at storytelling the traditional sort many years after the event i can still see in my mind's eye an elderly lady my friend's mother retelling the battle of ? she makes the characters come alive her passion is that of an eye witness one to the events on the ? heath a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in bars the length and ? of scotland as i discussed it with a friend one night in ? a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional reservoir fact and fiction blend with ? role models warning stories ? magic and mystery br br my name is ? like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that stretches back through generations it produces stories within stories stories that evoke the impenetrable wonder of scotland its rugged mountains shrouded in ? the stuff of legend yet ? is rooted in reality this is what gives it its special charm it has a rough beauty and authenticity tempered with some of the finest gaelic singing you will ever hear br br ? angus visits his grandfather in hospital shortly before his death he burns with frustration part of him yearns to be in the twenty first century to hang out in ? but he is raised on the western ? among a gaelic speaking community br br yet there is a deeper conflict within him he yearns to know the truth the truth behind his ? ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last fateful journey to the ? of one of ? most ? mountains can the truth be told or is it all in stories br br in this story about stories we revisit bloody battles poisoned lovers the folklore of old and the sometimes more treacherous folklore of accepted truth in doing so we each connect with angus as he lives the story of his own life br br ? the ? pinnacle is probably the most honest unpretentious and genuinely beautiful film of scotland ever made like angus i got slightly annoyed with the pretext of hanging stories on more stories but also like angus i ? this once i saw the ? picture ' forget the box office pastiche of braveheart and its like you might even ? the justly famous ? of the wicker man to see a film that is true to scotland this one is probably unique if you maybe ? on it deeply enough you might even re evaluate the power of storytelling and the age old question of whether there are some truths that cannot be told but only experienced\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ8ao-KkQsyg",
        "outputId": "a1ac05cb-b931-4691-e70a-89d2cb86e92e"
      },
      "source": [
        "# Since vocabulary size is restricted to VOCAB_SIZE, no word index should be greater than VOCAB_SIZE\n",
        "max_index = max ( [max(review) for review in train_data] )\n",
        "\n",
        "print(\"VOCAB_SIZE          =\", VOCAB_SIZE)\n",
        "print(\"Max index of a word =\", max_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VOCAB_SIZE          = 20000\n",
            "Max index of a word = 19999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpSNHE5DQxR_",
        "outputId": "868d5994-0119-4be0-dce3-c799ee80903a"
      },
      "source": [
        "\n",
        "# Find and store the length of the longest review in the dataset\n",
        "max_train_length = max (len(review) for review in train_data)\n",
        "max_test_length = max (len(review) for review in test_data)\n",
        "\n",
        "max_length = max(max_train_length, max_test_length)\n",
        "print(\"Maximum length of a review in dataset:\", max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum length of a review in dataset: 2494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k24Wpp-iQ06R",
        "outputId": "417dd4ba-2193-4442-da35-9fd430f0f475"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Pad each review in the dataset till their length = max_length\n",
        "print(\"Padding the reviews...\")\n",
        "train_data = pad_sequences (train_data, maxlen=max_length)\n",
        "test_data = pad_sequences (test_data, maxlen=max_length)\n",
        "\n",
        "print(\"Shape of train_data =\", train_data.shape)\n",
        "print(\"Shape of test_data  =\", test_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padding the reviews...\n",
            "Shape of train_data = (25000, 2494)\n",
            "Shape of test_data  = (25000, 2494)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oep_18hrQ3qY",
        "outputId": "49d1e9b1-03f3-442c-97ad-f7002f50a678"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, Flatten, GlobalAveragePooling1D\n",
        "from keras.regularizers import l2\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = VOCAB_SIZE,\n",
        "                    output_dim = 128,\n",
        "                    input_length = max_length))\n",
        "model.add(Conv1D(filters = 128,\n",
        "                 kernel_size = 3,\n",
        "                 strides = 1,\n",
        "                 padding = 'valid',\n",
        "                 activation = 'relu',\n",
        "                ))\n",
        "model.add(MaxPooling1D(pool_size = 7))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(units = 32,\n",
        "                activation = 'relu'))\n",
        "model.add(Dense(units = 1,\n",
        "                activation = 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 2494, 128)         2560000   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 2492, 128)         49280     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 356, 128)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,613,441\n",
            "Trainable params: 2,613,441\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "999Qth6SQ6vD"
      },
      "source": [
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'binary_crossentropy',\n",
        "              metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYL--tNCQ9qI",
        "outputId": "ea7b7307-a7e3-49d8-e65b-2fa17fc2df31"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, TerminateOnNaN\n",
        "\n",
        "callback_list = [\n",
        "                 ModelCheckpoint(\n",
        "                     filepath = '/content/drive/My Drive/Personal/Sentiment Analysis/best_CNNmodel.hdf5',\n",
        "                     monitor = 'val_acc',\n",
        "                     verbose = 1,\n",
        "                     save_best_only = True,\n",
        "                     save_weights_only = False,\n",
        "                     mode = 'max',\n",
        "                     period = 1\n",
        "                 ),\n",
        "                 \n",
        "                 EarlyStopping(\n",
        "                    monitor = 'val_acc',\n",
        "                    patience = 20,\n",
        "                    verbose = 1,\n",
        "                    mode = 'max',\n",
        "                    baseline = 0.5,\n",
        "                    restore_best_weights = True\n",
        "                 ),\n",
        "\n",
        "                 ReduceLROnPlateau(\n",
        "                     monitor = 'val_loss',\n",
        "                     factor = 0.2,\n",
        "                     patience = 5,\n",
        "                     verbose = 1,\n",
        "                     mode = 'min',\n",
        "                     cooldown = 1,\n",
        "                     min_lr = 0\n",
        "                 ),\n",
        "\n",
        "                 TerminateOnNaN()\n",
        "]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I61S1NS3RB1o",
        "outputId": "4024e26a-7f6a-4d12-ddb8-36670275c3f9"
      },
      "source": [
        "\n",
        "history = model.fit(train_data,\n",
        "                    train_labels,\n",
        "                    epochs = 15,\n",
        "                    batch_size = 64,\n",
        "                    verbose = 1,\n",
        "                    callbacks = callback_list,\n",
        "                    validation_split = 0.00512,\n",
        "                    shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "389/389 [==============================] - 481s 1s/step - loss: 0.5291 - acc: 0.7187 - val_loss: 0.2752 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.91406, saving model to /content/drive/My Drive/Personal/Sentiment Analysis/best_CNNmodel.hdf5\n",
            "Epoch 2/15\n",
            "389/389 [==============================] - 466s 1s/step - loss: 0.1708 - acc: 0.9392 - val_loss: 0.2101 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.91406\n",
            "Epoch 3/15\n",
            "389/389 [==============================] - 470s 1s/step - loss: 0.0479 - acc: 0.9900 - val_loss: 0.2297 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.91406 to 0.92188, saving model to /content/drive/My Drive/Personal/Sentiment Analysis/best_CNNmodel.hdf5\n",
            "Epoch 4/15\n",
            "389/389 [==============================] - 458s 1s/step - loss: 0.0090 - acc: 0.9996 - val_loss: 0.2389 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.92188\n",
            "Epoch 5/15\n",
            "389/389 [==============================] - 457s 1s/step - loss: 0.0027 - acc: 0.9998 - val_loss: 0.2638 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.92188\n",
            "Epoch 6/15\n",
            "389/389 [==============================] - 454s 1s/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.2790 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.92188\n",
            "Epoch 7/15\n",
            "389/389 [==============================] - 452s 1s/step - loss: 3.8737e-04 - acc: 1.0000 - val_loss: 0.2864 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.92188\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 8/15\n",
            "389/389 [==============================] - 454s 1s/step - loss: 2.6602e-04 - acc: 1.0000 - val_loss: 0.2882 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.92188\n",
            "Epoch 9/15\n",
            "389/389 [==============================] - 454s 1s/step - loss: 2.4156e-04 - acc: 1.0000 - val_loss: 0.2895 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.92188\n",
            "Epoch 10/15\n",
            "389/389 [==============================] - 449s 1s/step - loss: 2.2495e-04 - acc: 1.0000 - val_loss: 0.2923 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.92188\n",
            "Epoch 11/15\n",
            "389/389 [==============================] - 447s 1s/step - loss: 2.0004e-04 - acc: 1.0000 - val_loss: 0.2950 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.92188\n",
            "Epoch 12/15\n",
            "389/389 [==============================] - 448s 1s/step - loss: 1.7732e-04 - acc: 1.0000 - val_loss: 0.2995 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.92188\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 13/15\n",
            "389/389 [==============================] - 448s 1s/step - loss: 1.6334e-04 - acc: 1.0000 - val_loss: 0.2993 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.92188\n",
            "Epoch 14/15\n",
            "389/389 [==============================] - 453s 1s/step - loss: 1.5635e-04 - acc: 1.0000 - val_loss: 0.2995 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.92188\n",
            "Epoch 15/15\n",
            "389/389 [==============================] - 457s 1s/step - loss: 1.5013e-04 - acc: 1.0000 - val_loss: 0.3004 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.92188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-Q73jSGRFLY",
        "outputId": "f8188d19-7597-4974-e91a-c6b3c71e6794"
      },
      "source": [
        "print(\"Best Validation Accuracy =\", max(history.history['val_acc'])*100)\n",
        "print(\"Best Training Accuracy =\", max(history.history['acc'])*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Validation Accuracy = 92.1875\n",
            "Best Training Accuracy = 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chhWyuAERI9E"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "plt.plot(epochs, train_acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training & Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRMC0q5cRL1g"
      },
      "source": [
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.plot(epochs, train_loss, 'r', label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training & Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAGgt-wUROYP",
        "outputId": "3e25ddaf-54c7-482e-ef04-4cd538c4d08f"
      },
      "source": [
        "test_metrics = model.evaluate(test_data, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 130s 166ms/step - loss: 0.3895 - acc: 0.8996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiFCHYnYRQ5g",
        "outputId": "b512d997-d17a-4876-a7b2-6ce356f6adcf"
      },
      "source": [
        "test_accuracy = test_metrics[1]\n",
        "test_loss = test_metrics[0]\n",
        "\n",
        "print(\"Test Accuracy :\", test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy : 0.8995599746704102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3nGHnIqRTd9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}